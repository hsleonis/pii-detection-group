{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL0iyHWQ8Esl"
      },
      "source": [
        "# **Import:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0k2LsWN7PQC"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nziTzy_FBXsT"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from simpletransformers.ner import NERModel, NERArgs\n",
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report as crp\n",
        "from sklearn.metrics import average_precision_score, roc_curve, confusion_matrix, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import collections\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import tempfile\n",
        "import warnings\n",
        "from dataclasses import asdict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import seqeval\n",
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        "    performance_measure\n",
        ")\n",
        "from simpletransformers.config.model_args import NERArgs\n",
        "from simpletransformers.config.utils import sweep_config_to_sweep_values\n",
        "from simpletransformers.losses.loss_utils import init_loss\n",
        "from simpletransformers.ner.ner_utils import (\n",
        "    InputExample,\n",
        "    LazyNERDataset,\n",
        "    convert_examples_to_features,\n",
        "    get_examples_from_df,\n",
        "    load_hf_dataset,\n",
        "    read_examples_from_file,\n",
        "    flatten_results,\n",
        ")\n",
        "\n",
        "from transformers import DummyObject, requires_backends\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from tqdm.auto import tqdm, trange\n",
        "from transformers import (\n",
        "    AlbertConfig,\n",
        "    AlbertForTokenClassification,\n",
        "    AlbertTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        "    BertConfig,\n",
        "    BertForTokenClassification,\n",
        "    BertTokenizer,\n",
        "    BertweetTokenizer,\n",
        "    BigBirdConfig,\n",
        "    BigBirdForTokenClassification,\n",
        "    BigBirdTokenizer,\n",
        "    CamembertConfig,\n",
        "    CamembertForTokenClassification,\n",
        "    CamembertTokenizer,\n",
        "    DebertaConfig,\n",
        "    DebertaForTokenClassification,\n",
        "    DebertaTokenizer,\n",
        "    DebertaV2Config,\n",
        "    DebertaV2ForTokenClassification,\n",
        "    DebertaV2Tokenizer,\n",
        "    DistilBertConfig,\n",
        "    DistilBertForTokenClassification,\n",
        "    DistilBertTokenizer,\n",
        "    ElectraConfig,\n",
        "    ElectraForTokenClassification,\n",
        "    ElectraTokenizer,\n",
        "    HerbertTokenizerFast,\n",
        "    LayoutLMConfig,\n",
        "    LayoutLMForTokenClassification,\n",
        "    LayoutLMTokenizer,\n",
        "    LayoutLMv2Config,\n",
        "    LayoutLMv2ForTokenClassification,\n",
        "    LayoutLMv2Tokenizer,\n",
        "    LongformerConfig,\n",
        "    LongformerForTokenClassification,\n",
        "    LongformerTokenizer,\n",
        "    MPNetConfig,\n",
        "    MPNetForTokenClassification,\n",
        "    MPNetTokenizer,\n",
        "    MobileBertConfig,\n",
        "    MobileBertForTokenClassification,\n",
        "    MobileBertTokenizer,\n",
        "    NystromformerConfig,\n",
        "    NystromformerForTokenClassification,\n",
        "    RemBertConfig,\n",
        "    RemBertForTokenClassification,\n",
        "    RemBertTokenizer,\n",
        "    RemBertTokenizerFast,\n",
        "    RobertaConfig,\n",
        "    RobertaForTokenClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    SqueezeBertConfig,\n",
        "    SqueezeBertForTokenClassification,\n",
        "    SqueezeBertTokenizer,\n",
        "    XLMConfig,\n",
        "    XLMForTokenClassification,\n",
        "    XLMTokenizer,\n",
        "    XLMRobertaConfig,\n",
        "    XLMRobertaForTokenClassification,\n",
        "    XLMRobertaTokenizer,\n",
        "    XLNetConfig,\n",
        "    XLNetForTokenClassification,\n",
        "    XLNetTokenizerFast,\n",
        ")\n",
        "from transformers.convert_graph_to_onnx import convert, quantize\n",
        "from torch.optim import AdamW\n",
        "from transformers.optimization import Adafactor\n",
        "from transformers.optimization import (\n",
        "    get_constant_schedule,\n",
        "    get_constant_schedule_with_warmup,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    get_cosine_schedule_with_warmup,\n",
        "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "    get_polynomial_decay_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import List, Optional, Type\n",
        "from seqeval.metrics.v1 import SCORES, _precision_recall_fscore_support\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "    wandb_available = True\n",
        "except ImportError:\n",
        "    wandb_available = False\n",
        "\n",
        "sns.set_theme()\n",
        "sns.set(font_scale=1)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "MODELS_WITHOUT_CLASS_WEIGHTS_SUPPORT = [\"squeezebert\", \"deberta\", \"mpnet\"]\n",
        "\n",
        "MODELS_WITH_EXTRA_SEP_TOKEN = [\n",
        "    \"roberta\",\n",
        "    \"camembert\",\n",
        "    \"xlmroberta\",\n",
        "    \"longformer\",\n",
        "    \"mpnet\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4XzPm3Kq5SvO"
      },
      "outputs": [],
      "source": [
        "def precision_recall_fscore_support(y_true: List[List[str]],\n",
        "                                    y_pred: List[List[str]],\n",
        "                                    *,\n",
        "                                    average: Optional[str] = None,\n",
        "                                    warn_for=('precision', 'recall', 'f-score'),\n",
        "                                    beta: float = 1.0,\n",
        "                                    sample_weight: Optional[List[int]] = None,\n",
        "                                    zero_division: str = 'warn',\n",
        "                                    suffix: bool = False) -> SCORES:\n",
        "    \"\"\"Compute precision, recall, F-measure and support for each class.\n",
        "\n",
        "    Args:\n",
        "        y_true : 2d array. Ground truth (correct) target values.\n",
        "\n",
        "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
        "\n",
        "        beta : float, 1.0 by default\n",
        "            The strength of recall versus precision in the F-score.\n",
        "\n",
        "        average : string, [None (default), 'micro', 'macro', 'weighted']\n",
        "            If ``None``, the scores for each class are returned. Otherwise, this\n",
        "            determines the type of averaging performed on the data:\n",
        "            ``'micro'``:\n",
        "                Calculate metrics globally by counting the total true positives,\n",
        "                false negatives and false positives.\n",
        "            ``'macro'``:\n",
        "                Calculate metrics for each label, and find their unweighted\n",
        "                mean.  This does not take label imbalance into account.\n",
        "            ``'weighted'``:\n",
        "                Calculate metrics for each label, and find their average weighted\n",
        "                by support (the number of true instances for each label). This\n",
        "                alters 'macro' to account for label imbalance; it can result in an\n",
        "                F-score that is not between precision and recall.\n",
        "\n",
        "        warn_for : tuple or set, for internal use\n",
        "            This determines which warnings will be made in the case that this\n",
        "            function is being used to return only one of its metrics.\n",
        "\n",
        "        sample_weight : array-like of shape (n_samples,), default=None\n",
        "            Sample weights.\n",
        "\n",
        "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
        "            Sets the value to return when there is a zero division:\n",
        "               - recall: when there are no positive labels\n",
        "               - precision: when there are no positive predictions\n",
        "               - f-score: both\n",
        "\n",
        "            If set to \"warn\", this acts as 0, but warnings are also raised.\n",
        "\n",
        "        suffix : bool, False by default.\n",
        "\n",
        "    Returns:\n",
        "        precision : float (if average is not None) or array of float, shape = [n_unique_labels]\n",
        "\n",
        "        recall : float (if average is not None) or array of float, , shape = [n_unique_labels]\n",
        "\n",
        "        fbeta_score : float (if average is not None) or array of float, shape = [n_unique_labels]\n",
        "\n",
        "        support : int (if average is not None) or array of int, shape = [n_unique_labels]\n",
        "            The number of occurrences of each label in ``y_true``.\n",
        "\n",
        "    Examples:\n",
        "        >>> from seqeval.metrics.sequence_labeling import precision_recall_fscore_support\n",
        "        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "        (0.5, 0.5, 0.5, 2)\n",
        "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
        "        (0.5, 0.5, 0.5, 2)\n",
        "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "        (0.5, 0.5, 0.5, 2)\n",
        "\n",
        "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
        "        supports instead of averaging:\n",
        "\n",
        "        >>> precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "        (array([0., 1.]), array([0., 1.]), array([0., 1.]), array([1, 1]))\n",
        "\n",
        "    Notes:\n",
        "        When ``true positive + false positive == 0``, precision is undefined;\n",
        "        When ``true positive + false negative == 0``, recall is undefined.\n",
        "        In such cases, by default the metric will be set to 0, as will f-score,\n",
        "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
        "        modified with ``zero_division``.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_tp_actual_correct(y_true, y_pred, suffix, *args):\n",
        "        entities_true = defaultdict(set)\n",
        "        entities_pred = defaultdict(set)\n",
        "        for type_name, start, end in get_entities(y_true, suffix):\n",
        "            entities_true[type_name].add((start, end))\n",
        "        for type_name, start, end in get_entities(y_pred, suffix):\n",
        "            entities_pred[type_name].add((start, end))\n",
        "\n",
        "        target_names = sorted(set(entities_true.keys()) | set(entities_pred.keys()))\n",
        "\n",
        "        tp_sum = np.array([], dtype=np.int32)\n",
        "        pred_sum = np.array([], dtype=np.int32)\n",
        "        true_sum = np.array([], dtype=np.int32)\n",
        "        for type_name in target_names:\n",
        "            entities_true_type = entities_true.get(type_name, set())\n",
        "            entities_pred_type = entities_pred.get(type_name, set())\n",
        "            tp_sum = np.append(tp_sum, len(entities_true_type & entities_pred_type))\n",
        "            pred_sum = np.append(pred_sum, len(entities_pred_type))\n",
        "            true_sum = np.append(true_sum, len(entities_true_type))\n",
        "\n",
        "        return pred_sum, tp_sum, true_sum\n",
        "\n",
        "    precision, recall, f_score, true_sum = _precision_recall_fscore_support(\n",
        "        y_true, y_pred,\n",
        "        average=average,\n",
        "        warn_for=warn_for,\n",
        "        beta=beta,\n",
        "        sample_weight=sample_weight,\n",
        "        zero_division=zero_division,\n",
        "        scheme=None,\n",
        "        suffix=suffix,\n",
        "        extract_tp_actual_correct=extract_tp_actual_correct\n",
        "    )\n",
        "\n",
        "    return precision, recall, f_score, true_sum\n",
        "\n",
        "\n",
        "def get_entities(seq, suffix=False):\n",
        "    \"\"\"Gets entities from sequence.\n",
        "\n",
        "    Args:\n",
        "        seq (list): sequence of labels.\n",
        "\n",
        "    Returns:\n",
        "        list: list of (chunk_type, chunk_start, chunk_end).\n",
        "\n",
        "    Example:\n",
        "        >>> from seqeval.metrics.sequence_labeling import get_entities\n",
        "        >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
        "        >>> get_entities(seq)\n",
        "        [('PER', 0, 1), ('LOC', 3, 3)]\n",
        "    \"\"\"\n",
        "\n",
        "    def _validate_chunk(chunk, suffix):\n",
        "        if chunk in ['O', 'B', 'I', 'E', 'S', 'L', 'U']:\n",
        "            return\n",
        "\n",
        "        if suffix:\n",
        "            if not chunk.endswith(('-B', '-I', '-E', '-S', '-L', '-U')):\n",
        "                warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
        "\n",
        "        else:\n",
        "            if not chunk.startswith(('B-', 'I-', 'E-', 'S-', 'L-', 'U-')):\n",
        "                warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
        "\n",
        "    # for nested list\n",
        "    if any(isinstance(s, list) for s in seq):\n",
        "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
        "\n",
        "    prev_tag = 'O'\n",
        "    prev_type = ''\n",
        "    begin_offset = 0\n",
        "    chunks = []\n",
        "    for i, chunk in enumerate(seq + ['O']):\n",
        "        _validate_chunk(chunk, suffix)\n",
        "\n",
        "        if suffix:\n",
        "            tag = chunk[-1]\n",
        "            type_ = chunk[:-1].rsplit('-', maxsplit=1)[0] or '_'\n",
        "        else:\n",
        "            tag = chunk[0]\n",
        "            type_ = chunk[1:].split('-', maxsplit=1)[-1] or '_'\n",
        "\n",
        "        if end_of_chunk(prev_tag, tag, prev_type, type_):\n",
        "            chunks.append((prev_type, begin_offset, i - 1))\n",
        "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
        "            begin_offset = i\n",
        "        prev_tag = tag\n",
        "        prev_type = type_\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
        "    \"\"\"Checks if a chunk ended between the previous and current word.\n",
        "\n",
        "    Args:\n",
        "        prev_tag: previous chunk tag.\n",
        "        tag: current chunk tag.\n",
        "        prev_type: previous type.\n",
        "        type_: current type.\n",
        "\n",
        "    Returns:\n",
        "        chunk_end: boolean.\n",
        "    \"\"\"\n",
        "    chunk_end = False\n",
        "\n",
        "    if prev_tag == 'E' or prev_tag == 'L':\n",
        "        chunk_end = True\n",
        "    if prev_tag == 'S' or prev_tag == 'U':\n",
        "        chunk_end = True\n",
        "\n",
        "    if prev_tag == 'B' and tag == 'B':\n",
        "        chunk_end = True\n",
        "    if prev_tag == 'B' and tag == 'S':\n",
        "        chunk_end = True\n",
        "    if prev_tag == 'B' and tag == 'U':\n",
        "        chunk_end = True\n",
        "    if prev_tag == 'B' and tag == 'O':\n",
        "        chunk_end = True\n",
        "    if prev_tag == 'I' and tag == 'B':\n",
        "        chunk_end = True\n",
        "    if prev_tag == 'I' and tag == 'S':\n",
        "        chunk_end = True\n",
        "    if prev_tag == 'I' and tag == 'U':\n",
        "        chunk_end = True\n",
        "    if prev_tag == 'I' and tag == 'O':\n",
        "        chunk_end = True\n",
        "\n",
        "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
        "        chunk_end = True\n",
        "\n",
        "    return chunk_end\n",
        "\n",
        "\n",
        "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
        "    \"\"\"Checks if a chunk started between the previous and current word.\n",
        "\n",
        "    Args:\n",
        "        prev_tag: previous chunk tag.\n",
        "        tag: current chunk tag.\n",
        "        prev_type: previous type.\n",
        "        type_: current type.\n",
        "\n",
        "    Returns:\n",
        "        chunk_start: boolean.\n",
        "    \"\"\"\n",
        "    chunk_start = False\n",
        "\n",
        "    if tag == 'B':\n",
        "        chunk_start = True\n",
        "    if tag == 'S':\n",
        "        chunk_start = True\n",
        "    if tag == 'U':\n",
        "        chunk_start = True\n",
        "\n",
        "    if prev_tag == 'E' and tag == 'E':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'L' and tag == 'L':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'E' and tag == 'I':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'L' and tag == 'I':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'S' and tag == 'E':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'U' and tag == 'L':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'S' and tag == 'I':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'U' and tag == 'I':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'O' and tag == 'E':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'O' and tag == 'L':\n",
        "        chunk_start = True\n",
        "    if prev_tag == 'O' and tag == 'I':\n",
        "        chunk_start = True\n",
        "\n",
        "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
        "        chunk_start = True\n",
        "\n",
        "    return chunk_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NqX8jswL6WyK"
      },
      "outputs": [],
      "source": [
        "# replace functions (monkeypatching)\n",
        "seqeval.metrics.sequence_labeling.precision_recall_fscore_support = precision_recall_fscore_support\n",
        "seqeval.metrics.sequence_labeling.get_entities = get_entities\n",
        "seqeval.metrics.sequence_labeling.end_of_chunk = end_of_chunk\n",
        "seqeval.metrics.sequence_labeling.start_of_chunk = start_of_chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHAm0YZU8J1X"
      },
      "source": [
        "# **Load Datasets (All):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Gnp2RCh3Sg2"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('wikipii_x.csv')\n",
        "df_train = df_train.sort_values(by=['doc_id','sentence_id'], ascending=True)\n",
        "df_test = pd.read_csv('wikipii_x.csv')\n",
        "df_test = df_test.sort_values(by=['doc_id','sentence_id'], ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ylIovYyG3XYT"
      },
      "outputs": [],
      "source": [
        "df_train['words'] = df_train['words'].astype('str')\n",
        "df_train['labels'] = df_train['labels'].astype('str')\n",
        "df_test['words'] = df_test['words'].astype('str')\n",
        "df_test['labels'] = df_test['labels'].astype('str')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFBe0eGqEyXc"
      },
      "source": [
        "# **NERUpGraded:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ubm5JpsjE2O-"
      },
      "outputs": [],
      "source": [
        "class NERupgraded(NERModel):\n",
        "    def __init__(self,\n",
        "        model_type,\n",
        "        model_name,\n",
        "        labels=None,\n",
        "        weight=None,\n",
        "        args=None,\n",
        "        use_cuda=True,\n",
        "        cuda_device=-1,\n",
        "        onnx_execution_provider=None, **kwargs,):\n",
        "\n",
        "        super().__init__(model_type,\n",
        "        model_name,\n",
        "        labels,\n",
        "        weight,\n",
        "        args,\n",
        "        use_cuda,\n",
        "        cuda_device,\n",
        "        onnx_execution_provider,\n",
        "        **kwargs,)\n",
        "\n",
        "        # create new lists\n",
        "        self.train_loss_list = []\n",
        "        self.eval_loss_list = []\n",
        "        self.test_loss_list = []\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_dataset,\n",
        "        output_dir,\n",
        "        show_running_loss=True,\n",
        "        eval_data=None,\n",
        "        test_data=None,\n",
        "        verbose=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Trains the model on train_dataset.\n",
        "\n",
        "        Utility function to be used by the train_model() method. Not intended to be used directly.\n",
        "        \"\"\"\n",
        "\n",
        "        model = self.model\n",
        "        args = self.args\n",
        "\n",
        "        tb_writer = SummaryWriter(log_dir=args.tensorboard_dir)\n",
        "        train_sampler = RandomSampler(train_dataset)\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler=train_sampler,\n",
        "            batch_size=args.train_batch_size,\n",
        "            num_workers=self.args.dataloader_num_workers,\n",
        "        )\n",
        "\n",
        "        t_total = (\n",
        "            len(train_dataloader)\n",
        "            // args.gradient_accumulation_steps\n",
        "            * args.num_train_epochs\n",
        "        )\n",
        "\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\n",
        "        optimizer_grouped_parameters = []\n",
        "        custom_parameter_names = set()\n",
        "        for group in self.args.custom_parameter_groups:\n",
        "            params = group.pop(\"params\")\n",
        "            custom_parameter_names.update(params)\n",
        "            param_group = {**group}\n",
        "            param_group[\"params\"] = [\n",
        "                p for n, p in model.named_parameters() if n in params\n",
        "            ]\n",
        "            optimizer_grouped_parameters.append(param_group)\n",
        "\n",
        "        for group in self.args.custom_layer_parameters:\n",
        "            layer_number = group.pop(\"layer\")\n",
        "            layer = f\"layer.{layer_number}.\"\n",
        "            group_d = {**group}\n",
        "            group_nd = {**group}\n",
        "            group_nd[\"weight_decay\"] = 0.0\n",
        "            params_d = []\n",
        "            params_nd = []\n",
        "            for n, p in model.named_parameters():\n",
        "                if n not in custom_parameter_names and layer in n:\n",
        "                    if any(nd in n for nd in no_decay):\n",
        "                        params_nd.append(p)\n",
        "                    else:\n",
        "                        params_d.append(p)\n",
        "                    custom_parameter_names.add(n)\n",
        "            group_d[\"params\"] = params_d\n",
        "            group_nd[\"params\"] = params_nd\n",
        "\n",
        "            optimizer_grouped_parameters.append(group_d)\n",
        "            optimizer_grouped_parameters.append(group_nd)\n",
        "\n",
        "        if not self.args.train_custom_parameters_only:\n",
        "            optimizer_grouped_parameters.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"params\": [\n",
        "                            p\n",
        "                            for n, p in model.named_parameters()\n",
        "                            if n not in custom_parameter_names\n",
        "                            and not any(nd in n for nd in no_decay)\n",
        "                        ],\n",
        "                        \"weight_decay\": args.weight_decay,\n",
        "                    },\n",
        "                    {\n",
        "                        \"params\": [\n",
        "                            p\n",
        "                            for n, p in model.named_parameters()\n",
        "                            if n not in custom_parameter_names\n",
        "                            and any(nd in n for nd in no_decay)\n",
        "                        ],\n",
        "                        \"weight_decay\": 0.0,\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        warmup_steps = math.ceil(t_total * args.warmup_ratio)\n",
        "        args.warmup_steps = (\n",
        "            warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n",
        "        )\n",
        "\n",
        "        if args.optimizer == \"AdamW\":\n",
        "            optimizer = AdamW(\n",
        "                optimizer_grouped_parameters,\n",
        "                lr=args.learning_rate,\n",
        "                eps=args.adam_epsilon,\n",
        "                betas=args.adam_betas,\n",
        "            )\n",
        "        elif args.optimizer == \"Adafactor\":\n",
        "            optimizer = Adafactor(\n",
        "                optimizer_grouped_parameters,\n",
        "                lr=args.learning_rate,\n",
        "                eps=args.adafactor_eps,\n",
        "                clip_threshold=args.adafactor_clip_threshold,\n",
        "                decay_rate=args.adafactor_decay_rate,\n",
        "                beta1=args.adafactor_beta1,\n",
        "                weight_decay=args.weight_decay,\n",
        "                scale_parameter=args.adafactor_scale_parameter,\n",
        "                relative_step=args.adafactor_relative_step,\n",
        "                warmup_init=args.adafactor_warmup_init,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"{} is not a valid optimizer class. Please use one of ('AdamW', 'Adafactor') instead.\".format(\n",
        "                    args.optimizer\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if args.scheduler == \"constant_schedule\":\n",
        "            scheduler = get_constant_schedule(optimizer)\n",
        "\n",
        "        elif args.scheduler == \"constant_schedule_with_warmup\":\n",
        "            scheduler = get_constant_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=args.warmup_steps\n",
        "            )\n",
        "\n",
        "        elif args.scheduler == \"linear_schedule_with_warmup\":\n",
        "            scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=args.warmup_steps,\n",
        "                num_training_steps=t_total,\n",
        "            )\n",
        "\n",
        "        elif args.scheduler == \"cosine_schedule_with_warmup\":\n",
        "            scheduler = get_cosine_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=args.warmup_steps,\n",
        "                num_training_steps=t_total,\n",
        "                num_cycles=args.cosine_schedule_num_cycles,\n",
        "            )\n",
        "\n",
        "        elif args.scheduler == \"cosine_with_hard_restarts_schedule_with_warmup\":\n",
        "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=args.warmup_steps,\n",
        "                num_training_steps=t_total,\n",
        "                num_cycles=args.cosine_schedule_num_cycles,\n",
        "            )\n",
        "\n",
        "        elif args.scheduler == \"polynomial_decay_schedule_with_warmup\":\n",
        "            scheduler = get_polynomial_decay_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=args.warmup_steps,\n",
        "                num_training_steps=t_total,\n",
        "                lr_end=args.polynomial_decay_schedule_lr_end,\n",
        "                power=args.polynomial_decay_schedule_power,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"{} is not a valid scheduler.\".format(args.scheduler))\n",
        "\n",
        "        if args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "\n",
        "        global_step = 0\n",
        "        training_progress_scores = None\n",
        "        tr_loss, logging_loss = 0.0, 0.0\n",
        "        model.zero_grad()\n",
        "        train_iterator = trange(\n",
        "            int(args.num_train_epochs), desc=\"Epoch\", disable=args.silent, mininterval=0\n",
        "        )\n",
        "        epoch_number = 0\n",
        "        best_eval_metric = None\n",
        "        early_stopping_counter = 0\n",
        "        steps_trained_in_current_epoch = 0\n",
        "        epochs_trained = 0\n",
        "\n",
        "        if args.model_name and os.path.exists(args.model_name):\n",
        "            try:\n",
        "                # set global_step to gobal_step of last saved checkpoint from model path\n",
        "                checkpoint_suffix = args.model_name.split(\"/\")[-1].split(\"-\")\n",
        "                if len(checkpoint_suffix) > 2:\n",
        "                    checkpoint_suffix = checkpoint_suffix[1]\n",
        "                else:\n",
        "                    checkpoint_suffix = checkpoint_suffix[-1]\n",
        "                global_step = int(checkpoint_suffix)\n",
        "                epochs_trained = global_step // (\n",
        "                    len(train_dataloader) // args.gradient_accumulation_steps\n",
        "                )\n",
        "                steps_trained_in_current_epoch = global_step % (\n",
        "                    len(train_dataloader) // args.gradient_accumulation_steps\n",
        "                )\n",
        "\n",
        "                logger.info(\n",
        "                    \"   Continuing training from checkpoint, will skip to saved global_step\"\n",
        "                )\n",
        "                logger.info(\"   Continuing training from epoch %d\", epochs_trained)\n",
        "                logger.info(\"   Continuing training from global step %d\", global_step)\n",
        "                logger.info(\n",
        "                    \"   Will skip the first %d steps in the current epoch\",\n",
        "                    steps_trained_in_current_epoch,\n",
        "                )\n",
        "            except ValueError:\n",
        "                logger.info(\"   Starting fine-tuning.\")\n",
        "\n",
        "        if args.evaluate_during_training:\n",
        "            training_progress_scores = self._create_training_progress_scores(**kwargs)\n",
        "        if args.wandb_project:\n",
        "            wandb.init(\n",
        "                project=args.wandb_project,\n",
        "                config={**asdict(args)},\n",
        "                **args.wandb_kwargs,\n",
        "            )\n",
        "            wandb.run._label(repo=\"simpletransformers\")\n",
        "            wandb.watch(self.model)\n",
        "            self.wandb_run_id = wandb.run.id\n",
        "\n",
        "        if self.args.fp16:\n",
        "            from torch.cuda import amp\n",
        "\n",
        "            scaler = amp.GradScaler()\n",
        "\n",
        "        for _ in train_iterator:\n",
        "            model.train()\n",
        "            if epochs_trained > 0:\n",
        "                epochs_trained -= 1\n",
        "                continue\n",
        "            train_iterator.set_description(\n",
        "                f\"Epoch {epoch_number + 1} of {args.num_train_epochs}\"\n",
        "            )\n",
        "            batch_iterator = tqdm(\n",
        "                train_dataloader,\n",
        "                desc=f\"Running Epoch {epoch_number} of {args.num_train_epochs}\",\n",
        "                disable=args.silent,\n",
        "                mininterval=0,\n",
        "            )\n",
        "            for step, batch in enumerate(batch_iterator):\n",
        "                if steps_trained_in_current_epoch > 0:\n",
        "                    steps_trained_in_current_epoch -= 1\n",
        "                    continue\n",
        "\n",
        "                inputs = self._get_inputs_dict(batch)\n",
        "\n",
        "                if self.args.fp16:\n",
        "                    with amp.autocast():\n",
        "                        loss, *_ = self._calculate_loss(\n",
        "                            model,\n",
        "                            inputs,\n",
        "                            loss_fct=self.loss_fct,\n",
        "                            num_labels=self.num_labels,\n",
        "                            args=self.args,\n",
        "                        )\n",
        "                else:\n",
        "                    loss, *_ = self._calculate_loss(\n",
        "                        model,\n",
        "                        inputs,\n",
        "                        loss_fct=self.loss_fct,\n",
        "                        num_labels=self.num_labels,\n",
        "                        args=self.args,\n",
        "                    )\n",
        "\n",
        "                if args.n_gpu > 1:\n",
        "                    loss = (\n",
        "                        loss.mean()\n",
        "                    )  # mean() to average on multi-gpu parallel training\n",
        "\n",
        "                current_loss = loss.item()\n",
        "\n",
        "                # training loss list\n",
        "                self.train_loss_list.append(current_loss)\n",
        "\n",
        "                if show_running_loss:\n",
        "                    batch_iterator.set_description(\n",
        "                        f\"Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f}\"\n",
        "                    )\n",
        "\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "                if self.args.fp16:\n",
        "                    scaler.scale(loss).backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    if self.args.fp16:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                    if args.optimizer == \"AdamW\":\n",
        "                        torch.nn.utils.clip_grad_norm_(\n",
        "                            model.parameters(), args.max_grad_norm\n",
        "                        )\n",
        "\n",
        "                    if self.args.fp16:\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                    else:\n",
        "                        optimizer.step()\n",
        "                    scheduler.step()  # Update learning rate schedule\n",
        "                    model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                        # Log metrics\n",
        "                        tb_writer.add_scalar(\n",
        "                            \"lr\", scheduler.get_last_lr()[0], global_step\n",
        "                        )\n",
        "                        tb_writer.add_scalar(\n",
        "                            \"loss\",\n",
        "                            (tr_loss - logging_loss) / args.logging_steps,\n",
        "                            global_step,\n",
        "                        )\n",
        "                        logging_loss = tr_loss\n",
        "                        wandb_log_data = {\n",
        "                                    \"Training loss\": current_loss,\n",
        "                                    \"lr\": scheduler.get_last_lr()[0],\n",
        "                                    \"global_step\": global_step,\n",
        "                                }\n",
        "\n",
        "\n",
        "                        if args.wandb_project or self.is_sweeping:\n",
        "                            wandb.log(\n",
        "                                wandb_log_data\n",
        "                            )\n",
        "\n",
        "                    if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                        # Save model checkpoint\n",
        "                        output_dir_current = os.path.join(\n",
        "                            output_dir, \"checkpoint-{}\".format(global_step)\n",
        "                        )\n",
        "\n",
        "                        self.save_model(\n",
        "                            output_dir_current, optimizer, scheduler, model=model\n",
        "                        )\n",
        "\n",
        "                    if args.evaluate_during_training and (\n",
        "                        args.evaluate_during_training_steps > 0\n",
        "                        and global_step % args.evaluate_during_training_steps == 0\n",
        "                    ):\n",
        "                        output_dir_current = os.path.join(\n",
        "                            output_dir, \"checkpoint-{}\".format(global_step)\n",
        "                        )\n",
        "\n",
        "                        os.makedirs(output_dir_current, exist_ok=True)\n",
        "\n",
        "                        # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results, _, _ = self.eval_model(\n",
        "                            eval_data,\n",
        "                            verbose=verbose and args.evaluate_during_training_verbose,\n",
        "                            wandb_log=False,\n",
        "                            output_dir=output_dir_current,\n",
        "                            **kwargs,\n",
        "                        )\n",
        "\n",
        "                        # Save results to eval loss list\n",
        "                        self.eval_loss_list.append(results)\n",
        "\n",
        "                        if args.save_eval_checkpoints:\n",
        "                            self.save_model(\n",
        "                                output_dir_current,\n",
        "                                optimizer,\n",
        "                                scheduler,\n",
        "                                model=model,\n",
        "                                results=results,\n",
        "                            )\n",
        "\n",
        "                        training_progress_scores[\"global_step\"].append(global_step)\n",
        "                        training_progress_scores[\"train_loss\"].append(current_loss)\n",
        "                        for key in results:\n",
        "                            training_progress_scores[key].append(results[key])\n",
        "\n",
        "                        if test_data is not None:\n",
        "                            test_results, _, _ = self.eval_model(\n",
        "                                test_data,\n",
        "                                verbose=verbose\n",
        "                                and args.evaluate_during_training_verbose,\n",
        "                                silent=args.evaluate_during_training_silent,\n",
        "                                wandb_log=False,\n",
        "                                **kwargs,\n",
        "                            )\n",
        "                            for key in test_results:\n",
        "                                training_progress_scores[\"test_\" + key].append(\n",
        "                                    test_results[key]\n",
        "                                )\n",
        "\n",
        "                        report = pd.DataFrame(training_progress_scores)\n",
        "                        report.to_csv(\n",
        "                            os.path.join(\n",
        "                                args.output_dir, \"training_progress_scores.csv\"\n",
        "                            ),\n",
        "                            index=False,\n",
        "                        )\n",
        "\n",
        "                        if args.wandb_project or self.is_sweeping:\n",
        "                            wandb.log(self._get_last_metrics(training_progress_scores))\n",
        "\n",
        "                        for key, value in flatten_results(\n",
        "                            self._get_last_metrics(training_progress_scores)\n",
        "                        ).items():\n",
        "                            try:\n",
        "                                tb_writer.add_scalar(key, value, global_step)\n",
        "                            except (NotImplementedError, AssertionError):\n",
        "                                if verbose:\n",
        "                                    logger.warning(\n",
        "                                        f\"can't log value of type: {type(value)} to tensorboar\"\n",
        "                                    )\n",
        "                        tb_writer.flush()\n",
        "\n",
        "                        if not best_eval_metric:\n",
        "                            best_eval_metric = results[args.early_stopping_metric]\n",
        "                            self.save_model(\n",
        "                                args.best_model_dir,\n",
        "                                optimizer,\n",
        "                                scheduler,\n",
        "                                model=model,\n",
        "                                results=results,\n",
        "                            )\n",
        "                        if best_eval_metric and args.early_stopping_metric_minimize:\n",
        "                            if (\n",
        "                                results[args.early_stopping_metric] - best_eval_metric\n",
        "                                < args.early_stopping_delta\n",
        "                            ):\n",
        "                                best_eval_metric = results[args.early_stopping_metric]\n",
        "                                self.save_model(\n",
        "                                    args.best_model_dir,\n",
        "                                    optimizer,\n",
        "                                    scheduler,\n",
        "                                    model=model,\n",
        "                                    results=results,\n",
        "                                )\n",
        "                                early_stopping_counter = 0\n",
        "                            else:\n",
        "                                if args.use_early_stopping:\n",
        "                                    if (\n",
        "                                        early_stopping_counter\n",
        "                                        < args.early_stopping_patience\n",
        "                                    ):\n",
        "                                        early_stopping_counter += 1\n",
        "                                        if verbose:\n",
        "                                            logger.info(\n",
        "                                                f\" No improvement in {args.early_stopping_metric}\"\n",
        "                                            )\n",
        "                                            logger.info(\n",
        "                                                f\" Current step: {early_stopping_counter}\"\n",
        "                                            )\n",
        "                                            logger.info(\n",
        "                                                f\" Early stopping patience: {args.early_stopping_patience}\"\n",
        "                                            )\n",
        "                                    else:\n",
        "                                        if verbose:\n",
        "                                            logger.info(\n",
        "                                                f\" Patience of {args.early_stopping_patience} steps reached\"\n",
        "                                            )\n",
        "                                            logger.info(\" Training terminated.\")\n",
        "                                            train_iterator.close()\n",
        "                                        return (\n",
        "                                            global_step,\n",
        "                                            tr_loss / global_step\n",
        "                                            if not self.args.evaluate_during_training\n",
        "                                            else training_progress_scores,\n",
        "                                        )\n",
        "                        else:\n",
        "                            if (\n",
        "                                results[args.early_stopping_metric] - best_eval_metric\n",
        "                                > args.early_stopping_delta\n",
        "                            ):\n",
        "                                best_eval_metric = results[args.early_stopping_metric]\n",
        "                                self.save_model(\n",
        "                                    args.best_model_dir,\n",
        "                                    optimizer,\n",
        "                                    scheduler,\n",
        "                                    model=model,\n",
        "                                    results=results,\n",
        "                                )\n",
        "                                early_stopping_counter = 0\n",
        "                            else:\n",
        "                                if args.use_early_stopping:\n",
        "                                    if (\n",
        "                                        early_stopping_counter\n",
        "                                        < args.early_stopping_patience\n",
        "                                    ):\n",
        "                                        early_stopping_counter += 1\n",
        "                                        if verbose:\n",
        "                                            logger.info(\n",
        "                                                f\" No improvement in {args.early_stopping_metric}\"\n",
        "                                            )\n",
        "                                            logger.info(\n",
        "                                                f\" Current step: {early_stopping_counter}\"\n",
        "                                            )\n",
        "                                            logger.info(\n",
        "                                                f\" Early stopping patience: {args.early_stopping_patience}\"\n",
        "                                            )\n",
        "                                    else:\n",
        "                                        if verbose:\n",
        "                                            logger.info(\n",
        "                                                f\" Patience of {args.early_stopping_patience} steps reached\"\n",
        "                                            )\n",
        "                                            logger.info(\" Training terminated.\")\n",
        "                                            train_iterator.close()\n",
        "                                        return (\n",
        "                                            global_step,\n",
        "                                            tr_loss / global_step\n",
        "                                            if not self.args.evaluate_during_training\n",
        "                                            else training_progress_scores,\n",
        "                                        )\n",
        "                        model.train()\n",
        "\n",
        "            epoch_number += 1\n",
        "            output_dir_current = os.path.join(\n",
        "                output_dir, \"checkpoint-{}-epoch-{}\".format(global_step, epoch_number)\n",
        "            )\n",
        "\n",
        "            if args.save_model_every_epoch or args.evaluate_during_training:\n",
        "                os.makedirs(output_dir_current, exist_ok=True)\n",
        "\n",
        "            if args.save_model_every_epoch:\n",
        "                self.save_model(output_dir_current, optimizer, scheduler, model=model)\n",
        "\n",
        "            if args.evaluate_during_training and args.evaluate_each_epoch:\n",
        "                results, _, _ = self.eval_model(\n",
        "                    eval_data,\n",
        "                    verbose=verbose and args.evaluate_during_training_verbose,\n",
        "                    wandb_log=False,\n",
        "                    **kwargs,\n",
        "                )\n",
        "\n",
        "                # Save results to eval loss list\n",
        "                self.eval_loss_list.append(results)\n",
        "\n",
        "                self.save_model(\n",
        "                    output_dir_current, optimizer, scheduler, results=results\n",
        "                )\n",
        "\n",
        "                training_progress_scores[\"global_step\"].append(global_step)\n",
        "                training_progress_scores[\"train_loss\"].append(current_loss)\n",
        "                for key in results:\n",
        "                    training_progress_scores[key].append(results[key])\n",
        "\n",
        "                if test_data is not None:\n",
        "                    test_results, _, _ = self.eval_model(\n",
        "                        test_data,\n",
        "                        verbose=verbose and args.evaluate_during_training_verbose,\n",
        "                        silent=args.evaluate_during_training_silent,\n",
        "                        wandb_log=False,\n",
        "                        **kwargs,\n",
        "                    )\n",
        "\n",
        "                    # Save results to test loss list\n",
        "                    self.test_loss_list.append(test_results)\n",
        "\n",
        "                    for key in test_results:\n",
        "                        training_progress_scores[\"test_\" + key].append(\n",
        "                            test_results[key]\n",
        "                        )\n",
        "\n",
        "                report = pd.DataFrame(training_progress_scores)\n",
        "                report.to_csv(\n",
        "                    os.path.join(args.output_dir, \"training_progress_scores.csv\"),\n",
        "                    index=False,\n",
        "                )\n",
        "\n",
        "                if args.wandb_project or self.is_sweeping:\n",
        "                    wandb.log(self._get_last_metrics(training_progress_scores))\n",
        "\n",
        "                for key, value in flatten_results(\n",
        "                    self._get_last_metrics(training_progress_scores)\n",
        "                ).items():\n",
        "                    try:\n",
        "                        tb_writer.add_scalar(key, value, global_step)\n",
        "                    except (NotImplementedError, AssertionError):\n",
        "                        if verbose:\n",
        "                            logger.warning(\n",
        "                                f\"can't log value of type: {type(value)} to tensorboar\"\n",
        "                            )\n",
        "                tb_writer.flush()\n",
        "\n",
        "                if not best_eval_metric:\n",
        "                    best_eval_metric = results[args.early_stopping_metric]\n",
        "                    self.save_model(\n",
        "                        args.best_model_dir,\n",
        "                        optimizer,\n",
        "                        scheduler,\n",
        "                        model=model,\n",
        "                        results=results,\n",
        "                    )\n",
        "                if best_eval_metric and args.early_stopping_metric_minimize:\n",
        "                    if (\n",
        "                        results[args.early_stopping_metric] - best_eval_metric\n",
        "                        < args.early_stopping_delta\n",
        "                    ):\n",
        "                        best_eval_metric = results[args.early_stopping_metric]\n",
        "                        self.save_model(\n",
        "                            args.best_model_dir,\n",
        "                            optimizer,\n",
        "                            scheduler,\n",
        "                            model=model,\n",
        "                            results=results,\n",
        "                        )\n",
        "                        early_stopping_counter = 0\n",
        "                    else:\n",
        "                        if (\n",
        "                            args.use_early_stopping\n",
        "                            and args.early_stopping_consider_epochs\n",
        "                        ):\n",
        "                            if early_stopping_counter < args.early_stopping_patience:\n",
        "                                early_stopping_counter += 1\n",
        "                                if verbose:\n",
        "                                    logger.info(\n",
        "                                        f\" No improvement in {args.early_stopping_metric}\"\n",
        "                                    )\n",
        "                                    logger.info(\n",
        "                                        f\" Current step: {early_stopping_counter}\"\n",
        "                                    )\n",
        "                                    logger.info(\n",
        "                                        f\" Early stopping patience: {args.early_stopping_patience}\"\n",
        "                                    )\n",
        "                            else:\n",
        "                                if verbose:\n",
        "                                    logger.info(\n",
        "                                        f\" Patience of {args.early_stopping_patience} steps reached\"\n",
        "                                    )\n",
        "                                    logger.info(\" Training terminated.\")\n",
        "                                    train_iterator.close()\n",
        "                                return (\n",
        "                                    global_step,\n",
        "                                    tr_loss / global_step\n",
        "                                    if not self.args.evaluate_during_training\n",
        "                                    else training_progress_scores,\n",
        "                                )\n",
        "                else:\n",
        "                    if (\n",
        "                        results[args.early_stopping_metric] - best_eval_metric\n",
        "                        > args.early_stopping_delta\n",
        "                    ):\n",
        "                        best_eval_metric = results[args.early_stopping_metric]\n",
        "                        self.save_model(\n",
        "                            args.best_model_dir,\n",
        "                            optimizer,\n",
        "                            scheduler,\n",
        "                            model=model,\n",
        "                            results=results,\n",
        "                        )\n",
        "                        early_stopping_counter = 0\n",
        "                        early_stopping_counter = 0\n",
        "                    else:\n",
        "                        if (\n",
        "                            args.use_early_stopping\n",
        "                            and args.early_stopping_consider_epochs\n",
        "                        ):\n",
        "                            if early_stopping_counter < args.early_stopping_patience:\n",
        "                                early_stopping_counter += 1\n",
        "                                if verbose:\n",
        "                                    logger.info(\n",
        "                                        f\" No improvement in {args.early_stopping_metric}\"\n",
        "                                    )\n",
        "                                    logger.info(\n",
        "                                        f\" Current step: {early_stopping_counter}\"\n",
        "                                    )\n",
        "                                    logger.info(\n",
        "                                        f\" Early stopping patience: {args.early_stopping_patience}\"\n",
        "                                    )\n",
        "                            else:\n",
        "                                if verbose:\n",
        "                                    logger.info(\n",
        "                                        f\" Patience of {args.early_stopping_patience} steps reached\"\n",
        "                                    )\n",
        "                                    logger.info(\" Training terminated.\")\n",
        "                                    train_iterator.close()\n",
        "                                return (\n",
        "                                    global_step,\n",
        "                                    tr_loss / global_step\n",
        "                                    if not self.args.evaluate_during_training\n",
        "                                    else training_progress_scores,\n",
        "                                )\n",
        "\n",
        "        return (\n",
        "            global_step,\n",
        "            tr_loss / global_step\n",
        "            if not self.args.evaluate_during_training\n",
        "            else training_progress_scores,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuAyWTZFARqi"
      },
      "source": [
        "# **Model Trainer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GgdcyJOlAWos"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, df:pd.DataFrame, args:NERArgs = None, sim_model_id:str = '') -> None:\n",
        "        # Configure model\n",
        "        if args is None:\n",
        "            self.args = NERArgs()\n",
        "            self.args.evaluate_during_training = False\n",
        "            self.args.evaluate_during_training_steps = 3\n",
        "\n",
        "            self.args.num_train_epochs = 3\n",
        "            self.args.learning_rate = 1e-4\n",
        "            self.args.overwrite_output_dir = True\n",
        "            self.args.train_batch_size = 128\n",
        "            self.args.eval_batch_size = 128\n",
        "        else:\n",
        "            self.args = args\n",
        "\n",
        "        # split dataset on column\n",
        "        if not hasattr(self.args, 'split_column'):\n",
        "            self.args.split_column = 'doc_id'\n",
        "\n",
        "        # label column\n",
        "        if not hasattr(self.args, 'label_column'):\n",
        "            self.args.label_column = 'labels'\n",
        "\n",
        "        # set dataset\n",
        "        self.set_dataset(df)\n",
        "\n",
        "        # model id\n",
        "        self.sim_model_id = sim_model_id\n",
        "        self._model_id = ''\n",
        "\n",
        "        # model trainer & evaluationn flag\n",
        "        self._is_trained = False\n",
        "        self._is_evaluated = False\n",
        "        self.result = None\n",
        "\n",
        "\n",
        "    def set_dataset(self, df:pd.DataFrame) -> None:\n",
        "        # dataset\n",
        "        self.df = df\n",
        "\n",
        "        # labels list\n",
        "        if len(self.args.labels_list) == 0:\n",
        "            self.args.labels_list = self.df[self.args.label_column].unique().tolist()\n",
        "\n",
        "\n",
        "    def _train_test_split(self, ratio):\n",
        "        # split dataset on column\n",
        "        column = self.args.split_column\n",
        "\n",
        "        # percentage of all sentences\n",
        "        per = np.ceil((self.df[column].max() * ratio )/100)\n",
        "\n",
        "        return self.df[self.df[column] < per], self.df[self.df[column] >= per]\n",
        "\n",
        "\n",
        "    def set_model(self, model_id:str) -> None:\n",
        "        # huggingface model id\n",
        "        self._model_id = model_id\n",
        "\n",
        "        # create single word name\n",
        "        if self.sim_model_id != '':\n",
        "          self._model_name = self.sim_model_id\n",
        "        else:\n",
        "          self._model_name = self._model_id.split('-')[0]\n",
        "\n",
        "        # run without cuda if unavailable\n",
        "        if torch.cuda.is_available():\n",
        "            cuda_is_available = True\n",
        "        else:\n",
        "            cuda_is_available = False\n",
        "\n",
        "        # initiate model\n",
        "        self.model = NERupgraded(self._model_name,\n",
        "                                 self._model_id,\n",
        "                                 labels = self.args.labels_list,\n",
        "                                 args = self.args,\n",
        "                                 use_cuda = cuda_is_available)\n",
        "\n",
        "\n",
        "    def train(self, model_id:str, ratio=70):\n",
        "        # training data, tast data\n",
        "        self.train_data, self.test_data = self._train_test_split(ratio)\n",
        "\n",
        "        # set model\n",
        "        if self._model_id != model_id or not hasattr(self, 'model'):\n",
        "            self.set_model(model_id)\n",
        "\n",
        "        # train model\n",
        "        if self.args.evaluate_during_training:\n",
        "            column = self.args.split_column\n",
        "            eval_start_sent_id = random.randint(self.test_data[column].max(), self.test_data[column].min())\n",
        "\n",
        "            training_output = self.model.train_model(self.train_data,\n",
        "                                              eval_data = self.test_data.iloc[eval_start_sent_id:20])\n",
        "        else:\n",
        "            training_output = self.model.train_model(self.train_data)\n",
        "\n",
        "        if training_output is not None:\n",
        "            self._is_trained = True\n",
        "\n",
        "        return training_output\n",
        "\n",
        "\n",
        "    def _show_error_notice(self, notice):\n",
        "        if notice == \"not_trained\":\n",
        "            if not self._is_trained:\n",
        "                raise ValueError(\"=> The model needs to be trained. Use: `model.train()`, example: `model.train('bert-base-cased', ratio=70)`\")\n",
        "\n",
        "        if notice == \"not_eval\":\n",
        "            if not self._is_evaluated:\n",
        "                raise ValueError(\"=> The model needs to be evaluated. Use: `model.evaluate()`\")\n",
        "\n",
        "        if notice == \"eval_data\":\n",
        "            if not hasattr(self, \"eval_data\"):\n",
        "                raise ValueError(\"=> Evaluation dataset not found.\")\n",
        "\n",
        "\n",
        "    def eval(self, split:int = 1):\n",
        "        if self._show_error_notice(\"not_trained\"):\n",
        "            return None\n",
        "\n",
        "        prev = 0\n",
        "        for i in range(1, split+1):\n",
        "            next = int(np.ceil((len(self.test_data)*i*10)/100))\n",
        "            print(\"-\"*3, \"Evaluating segment: \", i, \", starting index:\", prev, \", ending index:\", next, \"-\"*3)\n",
        "\n",
        "            result, model_outputs, predictions = zip(self.model.eval_model(self.test_data.iloc[prev:next]))\n",
        "\n",
        "            self.result = result\n",
        "\n",
        "            if self.result is not None:\n",
        "                self._is_evaluated = True\n",
        "\n",
        "            prev = next\n",
        "\n",
        "        return predictions, model_outputs\n",
        "\n",
        "\n",
        "    def get_eval_result(self):\n",
        "        if self._show_error_notice(\"not_eval\"):\n",
        "            return None\n",
        "\n",
        "        return self.result[0]\n",
        "\n",
        "\n",
        "    def _prepapre_for_visual(self, predictions, model_outputs):\n",
        "        preds = [tag for pred_out in predictions for tag in pred_out]\n",
        "        outputs = [\n",
        "            np.mean(logits, axis=0) for output in model_outputs for logits in output\n",
        "        ]\n",
        "\n",
        "        return preds, outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjhAij2B9JOL"
      },
      "source": [
        "# **Train Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x01u5aoVPQE5"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "model = ModelTrainer(df_train, sim_model_id='bert')\n",
        "bert_70 = model.train('bert-base-cased', ratio=70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKWIqmyLPbzW"
      },
      "source": [
        "### **Test**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvZKyyEOPWKu"
      },
      "outputs": [],
      "source": [
        "result, out, pred = model.model.eval_model(df_test.iloc[:], cr = classification_report)\n",
        "\n",
        "tmp_preds = [tag for pred_out in pred for tag in pred_out]\n",
        "tmp_outputs = [\n",
        "    np.mean(logits, axis=0) for output in out for logits in output\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction**:"
      ],
      "metadata": {
        "id": "kgxKB9Cuz6uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, raw_outputs = model.predict([\"Sample sentence.\"])"
      ],
      "metadata": {
        "id": "lVkTc6Euz6H-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RL0iyHWQ8Esl",
        "IHAm0YZU8J1X",
        "VFBe0eGqEyXc",
        "BuAyWTZFARqi",
        "GjhAij2B9JOL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}